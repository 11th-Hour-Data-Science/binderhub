#!/usr/bin/env python3
"""
Script to install jupyterhub helm chart with minikube

for testing binderhub

Gets the jupyterhub chart version from the binderhub helm chart
to ensure we are testing against a reasonable version.
"""
import sys
import os
import pipes
from subprocess import check_call, check_output
import time

from kubernetes import client, config
from ruamel import yaml
from tempfile import NamedTemporaryFile

auth_enabled = '--auth' in sys.argv

namespace = os.environ.get('K8S_NAMESPACE', 'binderhub-test')
helm_release_name = 'binderhub-test'

here = os.path.abspath(os.path.dirname(__file__))
helm_chart = os.path.join(here, os.pardir, os.pardir, 'helm-chart')
requirements_yaml = os.path.join(helm_chart, 'binderhub', 'requirements.yaml')
values_yaml = os.path.join(helm_chart, 'binderhub', 'values.yaml')

def get_hub_chart_dependency():
    """Get the JupyterHub chart info from the binderhub chart requirements.yaml"""
    with open(requirements_yaml) as f:
        requirements = yaml.safe_load(f)
    for dep in requirements['dependencies']:
        if dep['name'] == 'jupyterhub':
            return dep
    else:
        raise ValueError("Couldn't find JupyterHub in %s:\n%s" %
            (requirements_yaml, requirements)
        )

jupyterhub = get_hub_chart_dependency()

# update the helm repo
check_call(['helm', 'repo', 'add', 'jupyterhub', jupyterhub['repository']])
check_call(['helm', 'repo', 'update', 'jupyterhub'])

# Deploying BinderHub normally automatically deploys JupyterHub from the same
# configuration file.
# In the CI tests JupyterHub is configured independently, so extract the
# JupyterHub config from the BinderHub helm chart values.yaml
tmp = NamedTemporaryFile(suffix='.yaml', delete=False)
tmp.close()
jupyterhub_values_yaml = tmp.name
with open(values_yaml) as valuesin:
    jupyterhub_values = yaml.safe_load(valuesin)['jupyterhub']
    with open(jupyterhub_values_yaml, 'w') as valuesout:
        yaml.safe_dump(jupyterhub_values, valuesout)

# launch with helm install (or upgrade, if already installed)
args = [
    'jupyterhub/jupyterhub',
    f'--version={jupyterhub["version"]}',
    f'--namespace={namespace}',
    '-f', jupyterhub_values_yaml,
    '-f', os.path.join(here, 'jupyterhub-chart-config.yaml'),
]
if auth_enabled:
    print('\nAuthentication is enabled')
    auth_conf_file = os.path.join(here, 'jupyterhub-chart-config-auth-additions.yaml')
    args.extend(['-f', auth_conf_file])

cmd = ['helm', 'upgrade', '--install', helm_release_name]
cmd.extend(args)
print("\n    %s\n" % ' '.join(map(pipes.quote, cmd)))

check_call(cmd)


# wait for deployment to be running via kube API

config.load_kube_config()
kube = client.CoreV1Api()

def wait_for(f, msg, timeout=120):
    """Wait for f() to return True"""
    print(f"Waiting until {msg}")
    for i in range(int(timeout)):
        if f():
            break
        time.sleep(1)
    else:
        raise TimeoutError(f"{msg} did not occur in {timeout} seconds")
    print(msg)

def pods_ready():
    """
    Return whether all pods in our test namespace are ready, which is a tougher
    criteria than running.

    ref: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#pod-conditions
    ref: https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.16/#podstatus-v1-core
    """
    pods = kube.list_namespaced_pod(namespace).items

    # FIXME: we could perhaps delegate this waiting for readiness to kubernetes
    # api and do like in z2jh:
    # https://github.com/jupyterhub/zero-to-jupyterhub-k8s/blob/5703a8de9017d83242f8d4dd1ac00887c162629b/dev#L261-L266
    all_pods_ready = True
    for pod in pods:
        if not any(
            condition.type == "Ready" and condition.status == "True"
            for condition in pod.status.conditions
        ):
            all_pods_ready = False
            print(f"{pod.status.phase}\t{pod.metadata.name}")
    return all_pods_ready

# wait until all of our pods are running and ready
try:
    wait_for(pods_ready, "Hub is up")
except TimeoutError:
    # show pods on timeout, in case there's a hint about what's wrong
    check_call(['kubectl', 'get', 'pod', '--all-namespaces'])
    not_running_pods = check_output([
        'kubectl', 'get', 'pods',
        '--no-headers',
        '--field-selector=status.phase!=Running',
        '--output=jsonpath={.items[*].metadata.name}',
    ], text=True).split(" ")

    # try print logs of not running pods
    for not_running_pod in not_running_pods:
        print()
        print(f"Logs for not running pod: {not_running_pod}")
        check_call(['kubectl', 'logs', not_running_pod, '--all-namespaces'])

    raise

os.remove(jupyterhub_values_yaml)
