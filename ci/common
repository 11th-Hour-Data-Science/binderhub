#!/bin/bash
# Use https://www.shellcheck.net/ to reduce mistakes if you make changes to this file.

remove_docker_mirror_on_travis() {
    # This is a workaround to an issue caused by the existence of a docker
    # registry mirror in our CI environment. Without this fix that removes the
    # mirror, chartpress fails to realize the existence of already built images
    # and rebuilds them.
    #
    # ref: https://github.com/moby/moby/issues/39120
    sudo cat /etc/docker/daemon.json
    echo '{"mtu": 1460}' | sudo dd of=/etc/docker/daemon.json
    sudo systemctl restart docker
    docker ps -a
}
export -f remove_docker_mirror_on_travis

setup_kubectl () {
    echo "setup kubectl"
    curl -Lo kubectl https://storage.googleapis.com/kubernetes-release/release/v${KUBE_VERSION}/bin/linux/amd64/kubectl
    chmod +x kubectl
    sudo mv kubectl /usr/local/bin/
}
export -f setup_kubectl

setup_helm () {
    echo "setup helm ${HELM_VERSION}"
    curl -sf https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 | DESIRED_VERSION=v${HELM_VERSION} bash
}
export -f setup_helm

setup_minikube () {
    # install conntrack for minikube with k8s 1.18.2
    # install libgnutls28-dev for pycurl
    sudo apt-get update
    sudo apt-get -y install conntrack libgnutls28-dev

    echo "setup minikube"
    curl -Lo minikube https://storage.googleapis.com/minikube/releases/v${MINIKUBE_VERSION}/minikube-linux-amd64
    chmod +x minikube
    sudo mv minikube /usr/local/bin/

    echo "start minikube"
    sudo CHANGE_MINIKUBE_NONE_USER=true minikube start --vm-driver=none --kubernetes-version=v${KUBE_VERSION}
    minikube update-context
}
export -f setup_minikube

await_jupyterhub() {
    kubectl rollout status --watch --timeout 300s deployment/proxy --namespace ${K8S_NAMESPACE:-binderhub-test} \
 && kubectl rollout status --watch --timeout 300s deployment/hub --namespace ${K8S_NAMESPACE:-binderhub-test} \
 && (
        if kubectl get deploy/autohttps --namespace ${K8S_NAMESPACE:-binderhub-test} &> /dev/null; then
            kubectl rollout status --watch --timeout 300s deployment/autohttps --namespace ${K8S_NAMESPACE:-binderhub-test} || exit 1
        fi
    )
}
export -f await_jupyterhub

await_binderhub() {
    await_jupyterhub
    kubectl rollout status --watch --timeout 300s deployment/binder --namespace ${K8S_NAMESPACE:-binderhub-test}
}
export -f await_binderhub

full_namespace_report () {
    # list config (secret,configmap)
    kubectl get secret,cm --namespace ${K8S_NAMESPACE:-binderhub-test}
    # list networking (service,ingress)
    kubectl get svc,ing --namespace ${K8S_NAMESPACE:-binderhub-test}
    # list workloads (deployment,statefulset,daemonset,pod)
    kubectl get deploy,sts,ds,pod --namespace ${K8S_NAMESPACE:-binderhub-test}

    # if any pod has any non-ready -> show its containers' logs
    kubectl get pods -o json --namespace ${K8S_NAMESPACE:-binderhub-test} \
    | jq '
        .items[]
        | select(
            any(.status.containerStatuses[]?; .ready == false)
        )
        | .metadata.name' \
    | xargs --max-args 1 --no-run-if-empty \
    sh -c 'printf "\nPod with non-ready container detected\n - Logs of $0:\n"; kubectl logs --all-containers $0 --namespace ${K8S_NAMESPACE:-binderhub-test}'

    # if any pods that should be scheduled by the user-scheduler are pending ->
    # show user-scheduler's logs
    (
        kubectl get pods -l "component in (user-placeholder,singleuser-server)" -o json --namespace ${K8S_NAMESPACE:-binderhub-test} \
        | jq -r '
            .items[]
            | select(.status.phase == "Pending")
            | .metadata.name
        '
    ) | xargs --max-args 1 --no-run-if-empty --max-lines \
    sh -c 'printf "\nPending user pod detected ($0)\n - Logs of deploy/user-scheduler:\n"; kubectl logs --all-containers deploy/user-scheduler --namespace ${K8S_NAMESPACE:-binderhub-test}'
}
export -f full_namespace_report
